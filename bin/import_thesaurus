#!/usr/bin/python3
# Read inscriptions from wiki dump and generate Elm module
#
# Import is based on two sources:
#
# 1. A wiki dump XML where we look for articles with a metastring starting
#    with "{{inscription"
# 2. A list that maps token names to unicode tokens
#
# From these sources an Elm module is generated that includes all inscriptions
# with the text of the inscriptions mapped according to the mapping list.
#
# Synopsis:
#    import_thesaurus MODULE_NAME XML_DUMP TOKEN_LIST
#
# Params:
#    MODULE_NAME    Name of the generated Elm module
#    XML_DUMP Wiki  Dump file to search inscriptions in
#    TOKEN_LIST     File with list of token names
#
# Example:
#
#    import_thesaurus \
#       Raetic raeticarum/univieacat_raetica-20200210-current.xml \
#       raetic.txt \

#       > src/Imported/RaeticInscriptions.elm

import sys
import os
import re
import html
import xml.etree.ElementTree as ET

from lib.tokens import Token, Types

def escape(str):
    """ I believe string literals in Elm have only the
    backslash and double quote as special chars.
    Prove me wrong. """
    return str.replace("\\", "\\\\").replace("\"", "\\\"")


class Raetic:
    """
    Information specific to the Raetic import
    """
    def __init__(self, tokens):
        self.reading_field = "reading_original"

        # Grouping Raetic inscriptions by alphabet seems most useful.
        self.group_field = "alphabet"

        types = Types(tokens)

        # Special chars used in the wiki
        types.add(Token('', '[', 'frag', '', 'ambi'))
        types.add(Token('', ']', 'frag', '', 'ambi'))
        types.add(Token('', '?', 'guess', '', 'ambi'))
        types.add(Token(' ', 'space', 'space', '', 'ambi'))
        types.add(Token('', 'punctuation1', 'punctuation', '', 'ambi'))
        types.add(Token('', 'punctuation2', 'punctuation', '', 'ambi'))
        types.add(Token('', 'punctuation3', 'punctuation', '', 'ambi'))
        types.add(Token('', 'punctuation4', 'punctuation', '', 'ambi'))
        types.add(Token('', 'punctuation5', 'punctuation', '', 'ambi'))
        # Since we don't have that sign, use divider 1
        types.add(Token('', 'punctuation7', 'punctuation', '', 'ambi'))

        # The wiki uses <br> tags for linebreaks
        types.add(Token("\n", '<br>', 'br', '', 'ambi'))

        # Chars having different names
        types.add(Token('', 'Ś', 'Ś', '', 'ambi'))
        types.add(Token('', 'Ś2', 'Ś', '', 'ambi'))
        types.add(Token('', 'Ś3', 'Ś', '', 'ambi'))
        types.add(Token('', 'Χ1', 'Χ', '', 'ambi'))
        types.add(Token('', 'Χ2', 'Χ', '', 'ambi'))
        types.add(Token('', 'Χ3', 'Χ', '', 'ambi'))
        types.add(Token('', '1d', '1d', '', 'ambi'))
        types.add(Token('', '10d', '10d', '', 'ambi'))

        # The punct ligatures are approximated by adding a period after
        # the respective sign. Using the Unicode Interpunct sign, which
        # will be ignored when searching.
        types.add(Token('·', 'Lpunct', 'Lpunct', '', 'sin'))
        types.add(Token('·', 'Lpunct', 'Lpunct', '', 'dex'))
        types.add(Token('·', 'Mpunct', 'Mpunct', '', 'sin'))
        types.add(Token('·', 'Mpunct', 'Mpunct', '', 'dex'))
        types.add(Token('·', 'Rpunct', 'Rpunct', '', 'sin'))
        types.add(Token('·', 'Rpunct', 'Rpunct', '', 'dex'))

        # Classes of chars we turn into wildcard chars because we don't
        # have the sign.
        prefix_map = {
            'line': "",
            'symbol': "",
            'add': "",
        }

        self.lookup = types.lookup(prefix_map)


class Lepontic:
    """
    Information specific to the Lepontic import
    """
    def __init__(self, tokens):
        self.reading_field = "reading_lepontic"
        self.group_field = "language"

        types = Types(tokens)

        # Special chars used in the wiki
        types.add(Token('', '[', 'frag', '', 'ambi'))
        types.add(Token('', ']', 'frag', '', 'ambi'))
        types.add(Token(' ', 'space', 'space', '', 'ambi'))
        types.add(Token('§', 'w', '§', '', 'ambi'))
        types.add(Token('!', '!', '!', '', 'ambi'))

        # The wiki uses <br> tags for linebreaks
        types.add(Token("\n", '<br>', 'br', '', 'ambi'))

        # Chars having different names
        #
        types.add(Token('', 'Ś', 'Ś', '', 'ambi'))   # Ð 1
        types.add(Token('', 'Ś2', 'Ś', '', 'ambi'))  # San2
        types.add(Token('', 'Ś3', 'Ś', '', 'ambi'))  # San3
        types.add(Token('', 'Ś5', 'Ś', '', 'ambi'))  # San5
        types.add(Token('', 'Ś6', 'Ś', '', 'ambi'))  # San6

        # They use "Χ" and we use "X", see the difference?
        # Hint, one of them is the Greek Chi.
        types.add(Token('', 'Χ', 'Χ', '', 'ambi'))   # X1
        types.add(Token('', 'Χ3', 'Χ', '', 'ambi'))  # X1
        types.add(Token('', 'Χ4', 'Χ', '', 'ambi'))  # X4
        types.add(Token('', 'Χ5', 'Χ', '', 'ambi'))  # X5
        types.add(Token('', 'Χ6', 'Χ', '', 'ambi'))  # X6

        types.add(Token('', 'addΦ1', 'addΦ1', '', 'ambi'))  # Q2
        types.add(Token('', 'addA1', 'addA1', '', 'sin'))   # A9?
        types.add(Token('', 'addA2', 'addA2', '', 'sin'))   # A6?
        types.add(Token('', 'addΘ1', 'addΘ1', '', 'ambi'))  # Θ1
        types.add(Token('', 'addT2', 'addT2', '', 'ambi'))  # T5 dex?

        types.add(Token('', 'Pd', 'Pd', '', 'ambi'))  # P1 dex

        # Couldn't find a matching short bar so I took the dot we have
        types.add(Token('', 'separator6', 'separator', '', 'ambi'))

        prefix_map = {}
        self.lookup = types.lookup(prefix_map)


class Reading:
    def __init__(self, script_name, tokens):
        try:
            strategy_class = globals()[script_name]
        except KeyError:
            raise ValueError(f"No strategy for {script_name}.")
        self.strategy = strategy_class(tokens)

        self.reading_str_pattern = re.compile(
            f"\|{self.strategy.reading_field}=(.*)\n"
        )

        # Tokens are encoded as "{{NAME[|SPEC_NAME]}}" but there are also
        # special markers [, ], and ?
        self.charlist_pattern = re.compile("\{\{([^}]+)}}|([][?])|(<br>)")
        self.direction_pattern = re.compile("\|direction=(.*)\n")
        self.script_pattern = re.compile("\|script=(.*)\n")
        self.group_pattern = re.compile(f"\|{self.strategy.group_field}=(.*)\n")

    def read(self, id, source):
        """
        Read inscription entry from wiki source.

        Params:
            id: Unique ID of the script
            source: Wiki source entry text.
            types: token lookup based on names

        Example of an inscription string from source:

            {{inscription
            |reading=)nuale!?&#93;n&#x0323;uale ri?ienalse!ri?ienals&#x0323;e&#x0323;
            |reading_original={{c|E}}{{c|S||d}}{{c|L}}{{c|A|A14}}{{c|N}}{{c|E}}{{c|I}}?{{c|I}}{{c|R|R2}}{{c|E}}{{c|L}}{{c|A|A14}}{{c|U}}{{c|N}}&#91;
            |direction=sinistroverse
            |letter_height_min=7
            |letter_height_max=11.5 cm
            |letter_number_min=15
            |line_number=1
            |script=North Italic script
            |alphabet=Magrè alphabet
            |language=Raetic
            |meaning=unknown
            |object=AK-1 rock
            |position=left
            |orientation=90
            |craftsmanship=engraved
            |condition=damaged
            |type_inscription=prob. votive
            |date=unknown
            |checklevel=0
            |disambiguation=AK-1
            }}

        The fields we are interested in are:

        - reading_...: Contains the inscription text
        - direction: contains the reading direction
        - script: used to filter out "Latin" inscriptions
        """

        if not source:
            raise ValueError("No text.")

        if not source.startswith("{{inscription"):
            raise ValueError("Not an inscription.")

        reading_str = self.reading_str_pattern.search(source).group(1)
        if not reading_str:
            raise ValueError("No field with reading.")

        # The text includes marker signs which are HTML-encoded
        # Example: &#91 -> [
        reading_decoded = html.unescape(reading_str)

        # Split into individual characters.
        char_strings = [ max(res) for res in
                         self.charlist_pattern.findall(reading_decoded) ]

        direction = self.direction_pattern.search(source).group(1)

        tokens = []
        for char_string in char_strings:
            # Put character parts into dict so we can do lookups with default.
            names = dict(enumerate(char_string.split("|")))

            search_names = []

            first_name = names.get(0)
            if first_name == 'c':
                # There are often two token names, the latter being more specific
                # Example: {{c|L|L2}}
                # We want to search for the more specific first.
                specific = names.get(2, None)
                if specific:
                    search_names.append(specific)
                general = names.get(1, None)
                if general:
                    search_names.append(general)
            else:
                search_names.append(first_name)


            dir = "sin"
            if names.get(3) == "d":
                dir = "dex"

            try:
                tokens.append(self.strategy.lookup.closest(search_names, dir))
            except KeyError:
                print(f"Missing {search_names} (writing direction {dir}) for inscription {id}", file=sys.stderr)

                # Signs that cannot be mapped are copied over verbatim
                name = search_names[0]
                if len(name) > 1:
                    tokens.append("<" + name + ">")
                else:
                    tokens.append(name)

        if direction == "dextroverse":
            dir = "LTR"
            text = "".join(tokens)
        else:
            dir = "RTL"
            # The text is entered reversed in the wiki to give
            # the desired visual appearance. Undo that by reversing the tokens.
            def split_by_line(tokens):
                line = []
                for t in tokens:
                    if t == "\n":
                        yield line
                        line = []
                    else:
                        line.append(t)
                if line:
                    yield line
            lines = ["".join(reversed(l)) for l in split_by_line(tokens)]
            text = "\n".join(lines)

        script = self.script_pattern.search(source).group(1)

        group_match = self.group_pattern.search(source)
        if group_match:
            group_str = group_match.group(1)
        else:
            group_str = "Unknown"

        # Take the first three letters of the alphabet name
        # for a short group name.
        group = group_str[0:3]

        return Inscription(id, group, dir, text, script)



class Inscription:
    """
    Represent Wiki entries.
    """
    def __init__(self, id, group, dir, text, script):
        self.id = id
        self.group = group
        self.dir = dir
        self.text = text
        self.script = script

    def elm_record(self, url_base):
        return """{{ id = "{}", group = "{}", dir = {}, plate = Nothing, text = \"\"\"{}\"\"\", link = Just "{}" }}""".format(
            escape(self.id),
            escape(self.group),
            self.dir,
            escape(self.text),
            escape(url_base + self.id)
        )

    def __lt__(self, other):
        '''
        sorts in human order on the token id
        '''
        return self._sort_words() < other._sort_words()

    def _sort_words(self):
        def atoi(text):
            return int(text) if text.isdigit() else text

        return [ atoi(c) for c in re.split(r'(\d+)', self.id) ]



script_name = sys.argv[1]
dump_file = sys.argv[2]
script_file = sys.argv[3]

with open(script_file) as script_lines:
    tokens = list(Token.from_lines(script_lines))

reading = Reading(script_name, tokens)

inscriptions = []
tree = ET.parse(dump_file)
root = tree.getroot()
xmlns = re.search(r"\{.*\}", root.tag).group()
base_url = os.path.dirname(root.find(f"*/{xmlns}base").text) + '/'

for page_elm in root.findall(f".//{xmlns}page"):
    title = page_elm.find(f"{xmlns}title").text
    text = page_elm.find(f".//{xmlns}text").text
    try:
        inscription = reading.read(title, text)

        # There are entries in latin script in the corpus, ignore those
        if inscription.script != "Latin script":
            inscriptions.append(inscription)

    except ValueError as e:
        pass

inscriptions.sort()

joined_inscriptions = "\n ,".join((i.elm_record(base_url) for i in inscriptions))

print(f"module Imported.{script_name}Inscriptions exposing (inscriptions)\nimport WritingDirections exposing (..)\ninscriptions =\n [{joined_inscriptions}\n ]\n")